{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "C:\\Users\\Konstantin-Asen\\AppData\\Local\\Temp\\ipykernel_4608\\2313528923.py:8: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init()\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from itertools import filterfalse\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_colbert.indexing import ColBERTIndexer\n",
    "from pyterrier_colbert.ranking import ColBERTFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ance.pyterrier_ance import ANCEIndexer, ANCETextScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfcorpus = pt.get_dataset(\"irds:beir/nfcorpus/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new NFCorpus index for BM25 and RM3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents: 100%|██████████| 3633/3633 [00:02<00:00, 1448.72it/s]\n"
     ]
    }
   ],
   "source": [
    "nfcorpus_index_src = os.path.abspath(\"nfcorpus-index\")\n",
    "if not os.path.exists(nfcorpus_index_src):\n",
    "    print(\"Creating a new NFCorpus index for BM25 and RM3...\")\n",
    "    pt.index.IterDictIndexer(nfcorpus_index_src, blocks=True, meta={\"docno\": 20, \"text\": 4096}).index(nfcorpus.get_corpus_iter(), fields=[\"docno\", \"text\"])\n",
    "\n",
    "nfcorpus_index = pt.IndexFactory.of(nfcorpus_index_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] opening zip file\n",
      "[INFO] [finished] opening zip file [2ms]\n",
      "[INFO] [starting] opening zip file\n",
      "[INFO] [finished] opening zip file [1ms]\n"
     ]
    }
   ],
   "source": [
    "queries = nfcorpus.get_topics(\"text\")\n",
    "qrels = nfcorpus.get_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.terrier.Retriever(nfcorpus_index, wmodel=\"BM25\", metadata=[\"docno\", \"text\"])\n",
    "rm3 = pt.rewrite.RM3(nfcorpus_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found. Creating a new NFCorpus index for ColBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents:   0%|          | 0/3633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 29, 15:42:50] [0] \t\t #> Local args.bsize = 128\n",
      "[Mar 29, 15:42:50] [0] \t\t #> args.index_root = c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-index\n",
      "[Mar 29, 15:42:50] [0] \t\t #> self.possible_subset_sizes = [1491308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 29, 15:42:52] #> Loading model checkpoint.\n",
      "[Mar 29, 15:42:52] #> Loading checkpoint c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\colbert_checkpoint\\colbert.dnn\n",
      "[Mar 29, 15:42:53] #> checkpoint['epoch'] = 0\n",
      "[Mar 29, 15:42:53] #> checkpoint['batch'] = 44500\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Mar 29, 15:42:53] #> Note: Output directory c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-index already exists\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Mar 29, 15:42:53] #> Creating directory c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-colbert-index \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents: 100%|██████████| 3633/3633 [00:03<00:00, 1132.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 29, 15:44:13] [0] \t\t [NOTE] Done with local share.\n",
      "[Mar 29, 15:44:13] [0] \t\t #> Joining saver thread.\n",
      "#> num_embeddings = 636776\n",
      "[Mar 29, 15:44:13] #> Starting..\n",
      "[Mar 29, 15:44:13] #> Processing slice #1 of 1 (range 0..1).\n",
      "[Mar 29, 15:44:13] #> Will write to c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-colbert-index\\ivfpq.100.faiss.\n",
      "[Mar 29, 15:44:13] #> Loading c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-colbert-index\\0.sample ...\n",
      "#> Sample has shape (31838, 128)\n",
      "[Mar 29, 15:44:13] #> Training with the vectors...\n",
      "[Mar 29, 15:44:13] #> Training now (using 0 GPUs)...\n",
      "0.39649462699890137\n",
      "[Mar 29, 15:44:14] Done training!\n",
      "\n",
      "[Mar 29, 15:44:14] #> Indexing the vectors...\n",
      "[Mar 29, 15:44:14] #> Loading ('c:\\\\Users\\\\Konstantin-Asen\\\\Desktop\\\\IR-research-project\\\\nfcorpus-colbert-index\\\\0.pt', None, None) (from queue)...\n",
      "[Mar 29, 15:44:14] #> Processing a sub_collection with shape (636776, 128)\n",
      "[Mar 29, 15:44:14] Add data with shape (636776, 128) (offset = 0)..\n",
      "[Mar 29, 15:44:14] Done indexing!\n",
      "[Mar 29, 15:44:14] Writing index to c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\nfcorpus-colbert-index\\ivfpq.100.faiss ...\n",
      "[Mar 29, 15:44:14] \n",
      "\n",
      "Done! All complete (for slice #1 of 1)!\n",
      "#> Faiss encoding complete\n",
      "#> Indexing complete, Time elapsed 84.21 seconds\n",
      "Index successfully created!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_url = \"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\"\n",
    "extract_dir = \"colbert_checkpoint\"\n",
    "checkpoint_path = \"colbert_checkpoint.zip\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(\"Downloading checkpoint...\")\n",
    "    wget.download(checkpoint_url, checkpoint_path)\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(checkpoint_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "colbert_checkpoint_path = os.path.abspath(\"colbert_checkpoint/colbert.dnn\")\n",
    "index_root = os.path.abspath(\"nfcorpus-index\")\n",
    "index_name = os.path.abspath(\"nfcorpus-colbert-index\")\n",
    "\n",
    "if not os.path.exists(index_name):\n",
    "    print(\"Index not found. Creating a new NFCorpus index for ColBERT...\")\n",
    "    colbert_index = ColBERTIndexer(\n",
    "        checkpoint=colbert_checkpoint_path,\n",
    "        index_root=index_root,\n",
    "        index_name=index_name,\n",
    "        chunksize=64, # Maybe even 128, the allowed maximum --> it regulates the size of PyTorch temp files that are created by the indexer\n",
    "        gpu=True # if the torch.cuda returned False, comment this\n",
    "    )\n",
    "    colbert_index.index(nfcorpus.get_corpus_iter())\n",
    "    print(\"Index successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 29, 15:44:25] #> Loading model checkpoint.\n",
      "[Mar 29, 15:44:25] #> Loading checkpoint c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\colbert_checkpoint\\colbert.dnn\n",
      "[Mar 29, 15:44:26] #> checkpoint['epoch'] = 0\n",
      "[Mar 29, 15:44:26] #> checkpoint['batch'] = 44500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\.venv\\Lib\\site-packages\\colbert\\utils\\amp.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "colbert_reranker = ColBERTFactory(colbert_checkpoint_path, index_root, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ance_extract_dir = \"ance_checkpoint\"\n",
    "ance_checkpoint_path = \"ance_checkpoint.zip\"\n",
    "\n",
    "if not os.path.exists(ance_extract_dir):\n",
    "    with zipfile.ZipFile(ance_checkpoint_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(ance_extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found. Creating a new NFCorpus index for ANCE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents:   0%|          | 0/3633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\ance_checkpoint\n",
      "Using mean: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\ance_checkpoint were not used when initializing RobertaDot_NLL_LN: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaDot_NLL_LN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaDot_NLL_LN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents:   1%|          | 29/3633 [00:01<02:57, 20.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir/nfcorpus/test documents: 100%|██████████| 3633/3633 [01:03<00:00, 56.93it/s]\n",
      "Indexing: 100%|██████████| 3633/3633 [01:01<00:00, 58.63d/s]\n",
      "Inferencing: 29it [01:02,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ance_checkpoint_path = os.path.abspath(\"ance_checkpoint\")\n",
    "ance_index_name = os.path.abspath(\"nfcorpus-ance-index\")\n",
    "\n",
    "if not os.path.exists(ance_index_name):\n",
    "    print(\"Index not found. Creating a new NFCorpus index for ANCE...\")\n",
    "    ance_index = ANCEIndexer(ance_checkpoint_path, ance_index_name, num_docs=3633)\n",
    "    ance_index.index(nfcorpus.get_corpus_iter())\n",
    "    print(\"Index successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\ance_checkpoint were not used when initializing RobertaDot_NLL_LN: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaDot_NLL_LN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaDot_NLL_LN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ance_reranker = ANCETextScorer(ance_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dict = {\n",
    "    \"BM25\": bm25,\n",
    "    \"BM25_RM3\": bm25 >> rm3 >> bm25,\n",
    "    \"BM25_COLBERT\": bm25 >> colbert_reranker.text_scorer(),\n",
    "    \"BM25_ANCE\": bm25 >> ance_reranker,\n",
    "    \"BM25_RM3_COLBERT\": bm25 >> rm3 >> bm25 >> colbert_reranker.text_scorer(),\n",
    "    \"BM25_RM3_ANCE\": bm25 >> rm3 >> bm25 >> ance_reranker,\n",
    "    \"BM25_COLBERT_RM3\": bm25 >> colbert_reranker.text_scorer() >> rm3 >> bm25,\n",
    "    \"BM25_COLBERT_ANCE\": bm25 >> colbert_reranker.text_scorer() >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "    \"BM25_ANCE_RM3\": bm25 >> ance_reranker >> rm3 >> bm25,\n",
    "    \"BM25_ANCE_COLBERT\": bm25 >> ance_reranker >> colbert_reranker.text_scorer(),\n",
    "    \"BM25_RM3_COLBERT_ANCE\": bm25 >> rm3 >> bm25 >> colbert_reranker.text_scorer() >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "    \"BM25_RM3_ANCE_COLBERT\": bm25 >> rm3 >> bm25 >> ance_reranker >> colbert_reranker.text_scorer(),\n",
    "    \"BM25_COLBERT_RM3_ANCE\": bm25 >> colbert_reranker.text_scorer() >> rm3 >> bm25 >> ance_reranker,\n",
    "    \"BM25_COLBERT_ANCE_RM3\": bm25 >> colbert_reranker.text_scorer() >> pt.text.get_text(nfcorpus_index) >> ance_reranker >> rm3 >> bm25,\n",
    "    \"BM25_ANCE_RM3_COLBERT\": bm25 >> ance_reranker >> rm3 >> bm25 >> colbert_reranker.text_scorer(),\n",
    "    \"BM25_ANCE_COLBERT_RM3\": bm25 >> ance_reranker >> colbert_reranker.text_scorer() >> rm3 >> bm25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Konstantin-Asen\\Desktop\\IR-research-project\\.venv\\Lib\\site-packages\\colbert\\utils\\amp.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else nullcontext()\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Inferencing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 3it [00:02,  1.17it/s]\n",
      "Inferencing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 932it [30:46,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ran for 220 minutes\n",
    "\n",
    "if not os.path.exists(\"nfcorpus-twofold/results.csv\"):\n",
    "    twofold_results = pt.Experiment(\n",
    "        [\n",
    "            bm25,\n",
    "            bm25 >> rm3 >> bm25,\n",
    "            bm25 >> colbert_reranker.text_scorer(),\n",
    "            bm25 >> ance_reranker\n",
    "        ],\n",
    "        queries,\n",
    "        qrels,\n",
    "        [\"map\", \"ndcg_cut_10\", \"recip_rank\", \"mrt\"],\n",
    "        [\"BM25\", \"BM25_RM3\", \"BM25_COLBERT\", \"BM25_ANCE\"],\n",
    "        save_dir=\"nfcorpus-twofold\",\n",
    "        save_mode=\"reuse\",\n",
    "        baseline=0,\n",
    "        correction=\"bonferroni\"\n",
    "    )\n",
    "    twofold_results.to_csv(\"nfcorpus-twofold/results.csv\", sep=',', na_rep=\"NaN\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran for ... minutes\n",
    "\n",
    "if not os.path.exists(\"nfcorpus-threefold/results.csv\"):\n",
    "    bm25_rm3 = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-twofold/BM25_RM3.res.gz\"), uniform=False)\n",
    "    bm25_colbert = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-twofold/BM25_COLBERT.res.gz\"), uniform=False)\n",
    "    bm25_ance = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-twofold/BM25_ANCE.res.gz\"), uniform=False)\n",
    "\n",
    "    threefold_results = pt.Experiment(\n",
    "        [\n",
    "            bm25,\n",
    "            bm25_rm3 >> pt.text.get_text(nfcorpus_index) >> colbert_reranker.text_scorer(),\n",
    "            bm25_rm3 >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "            bm25_colbert >> rm3 >> bm25,\n",
    "            bm25_colbert >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "            bm25_ance >> rm3 >> bm25,\n",
    "            bm25_ance >> pt.text.get_text(nfcorpus_index) >> colbert_reranker.text_scorer()\n",
    "        ],\n",
    "        queries,\n",
    "        qrels,\n",
    "        [\"map\", \"ndcg_cut_10\", \"recip_rank\", \"mrt\"],\n",
    "        [\"BM25\", \"BM25_RM3_COLBERT\", \"BM25_RM3_ANCE\", \"BM25_COLBERT_RM3\", \"BM25_COLBERT_ANCE\", \"BM25_ANCE_RM3\", \"BM25_ANCE_COLBERT\"],\n",
    "        save_dir=\"nfcorpus-threefold\",\n",
    "        save_mode=\"reuse\",\n",
    "        baseline=0,\n",
    "        correction=\"bonferroni\"\n",
    "    )\n",
    "    threefold_results.to_csv(\"nfcorpus-threefold/results.csv\", sep=',', na_rep=\"NaN\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran for ... minutes\n",
    "\n",
    "if not os.path.exists(\"nfcorpus-fourfold/results.csv\"):\n",
    "    bm25_rm3_colbert = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_RM3_COLBERT.res.gz\"), uniform=False)\n",
    "    bm25_rm3_ance = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_RM3_ANCE.res.gz\"), uniform=False)\n",
    "    bm25_colbert_rm3 = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_COLBERT_RM3.res.gz\"), uniform=False)\n",
    "    bm25_colbert_ance = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_COLBERT_ANCE.res.gz\"), uniform=False)\n",
    "    bm25_ance_rm3 = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_ANCE_RM3.res.gz\"), uniform=False)\n",
    "    bm25_ance_colbert = pt.Transformer.from_df(pt.io.read_results(\"nfcorpus-threefold/BM25_ANCE_COLBERT.res.gz\"), uniform=False)\n",
    "\n",
    "    fourfold_results = pt.Experiment(\n",
    "        [\n",
    "            bm25,\n",
    "            bm25_rm3_colbert >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "            bm25_rm3_ance >> pt.text.get_text(nfcorpus_index) >> colbert_reranker.text_scorer(),\n",
    "            bm25_colbert_rm3 >> pt.text.get_text(nfcorpus_index) >> ance_reranker,\n",
    "            bm25_colbert_ance >> rm3 >> bm25,\n",
    "            bm25_ance_rm3 >> pt.text.get_text(nfcorpus_index) >> colbert_reranker.text_scorer(),\n",
    "            bm25_ance_colbert >> rm3 >> bm25\n",
    "        ],\n",
    "        queries,\n",
    "        qrels,\n",
    "        [\"map\", \"ndcg_cut_10\", \"recip_rank\", \"mrt\"],\n",
    "        [\"BM25\", \"BM25_RM3_COLBERT_ANCE\", \"BM25_RM3_ANCE_COLBERT\", \"BM25_COLBERT_RM3_ANCE\", \"BM25_COLBERT_ANCE_RM3\", \"BM25_ANCE_RM3_COLBERT\", \"BM25_ANCE_COLBERT_RM3\"],\n",
    "        save_dir=\"nfcorpus-fourfold\",\n",
    "        save_mode=\"reuse\",\n",
    "        baseline=0,\n",
    "        correction=\"bonferroni\"\n",
    "    )\n",
    "    fourfold_results.to_csv(\"nfcorpus-fourfold/results.csv\", sep=',', na_rep=\"NaN\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
